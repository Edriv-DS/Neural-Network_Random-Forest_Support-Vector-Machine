# -*- coding: utf-8 -*-
"""9_19101289_MdMuballighHossainBhuyain.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19o5TFqU_bUEwNvRWTsnTeKBUekkP6zNP
"""

# Importing the required Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn as sk
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

# Mounting Drive to the Colab Notebook
from google.colab import drive
drive.mount('/content/drive')

# We will read our CSV file from our Google Drive and store it in a variable called df
df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Melanoma.csv')

#Viewing the shape and structure of our dataset/ counting rows and columns of the data set
df.shape

#Viewing a portion of the dataset to learn more about it
df.head()

# Counting the empty columns
df.isna().sum()

df.describe()

# We can see that all of our columns hold value, only anatom_site_general_challenge, sex and age_approx columns have a few null values.
# but that is trivial in comparison to 33126.
# So we are not dropping any columns. 
# However we can sort out the row values 
# -------------------  Dropping Null or Empty Values from Row. We do this to improve our training program. ------------------------------------------

# Counting Number of missing values in the 'Sex' Column
print("Null Values : ", df['sex'].isnull().sum())
# Creating a Subset
df_subset = df[df['sex'].notnull()]
# Printing Shape
print("Shape of dataframe before dropping:", df.shape)
# Dropping null values from the row
df = df.dropna(axis = 0, subset = ['sex'])
print("Shape after dropping:", df.shape)

# Counting Number of missing values in the 'anatom_site_general_challenge' Column
print("Null Values : ", df['anatom_site_general_challenge'].isnull().sum())
# Creating a Subset
df_subset = df[df['anatom_site_general_challenge'].notnull()]
# Printing Shape
print("Shape of dataframe before dropping:", df.shape)
# Dropping null values from the row
df = df.dropna(axis = 0, subset = ['anatom_site_general_challenge'])
print("Shape after dropping:", df.shape)

# -------------------  Imputing Values in our dataset. We do this to improve our training program. ------------------------------------------
# Imputing Values in the 'age_approx' column
from sklearn.impute import SimpleImputer
impute = SimpleImputer(missing_values=np.nan, strategy='mean')
impute.fit(df[['age_approx']])
df['age_approx'] = impute.transform(df[['age_approx']])
df.isna().sum() # checking if any column has any empty values

df.head() # To verify whether there is any empty column

# -------------------  Encoding Starts Here ------------------------------------------
# Counting Malignant and Benign Cells
df['benign_malignant'].value_counts()

# Encoding categorical features
from sklearn.preprocessing import LabelEncoder
encode = LabelEncoder()
# Encoding 'benign_malignant'
df.iloc[:,6] = encode.fit_transform(df.iloc[:,6].values)
df.iloc[:,6]

# Encoding 'sex'
df.iloc[:,2] = encode.fit_transform(df.iloc[:,2].values)
df.iloc[:,2]

# Encoding 'diagnosis'
df.iloc[:,5] = encode.fit_transform(df.iloc[:,5].values)
df.iloc[:,5]

# Encoding 'anatom_site_general_challenge'
df.iloc[:,4] = encode.fit_transform(df.iloc[:,4].values)
df.iloc[:,4]

df.head(5) # verifying if our encoding was successful

# Pair Plot 
sns.pairplot(df.iloc[:,2:7], hue='diagnosis')

# Visualizing correlation
sns.heatmap(df.iloc[:,2:11].corr(),annot=True, fmt='.0%')

# -------------------  Splitting Begins Here ------------------------------------------
# Splitting the dataset into independent X and dependent Y
x = df.drop(['benign_malignant','image_name','patient_id','patient_code'],axis=1)
y = df['benign_malignant']

#Train and Test Split --- > Train : 80%, Test : 20%
x_train, x_test, y_train, y_test = train_test_split(x,y, train_size=0.8, random_state=22)
print("Training dataset shape : ",x_train.shape)
print("Testing dataset shape : ",x_test.shape)
x_train.head()

# -------------------  Scaling Begins Here ------------------------------------------
# Scaling our data in order to remove biasness or deviance - Feature Scaling
# Method - Min Max Scaling Method
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
scaler.fit(x_train)

# Transforming
x_train_temp = scaler.transform(x_train)

# ---------------------  Support Vector Classifier ---------------------------------
from sklearn.svm import SVC
svc = SVC(kernel="linear")
svc.fit(x_train,y_train)
accuracy_train_01 = svc.score(x_train,y_train)
accuracy_test_01 = svc.score(x_test,y_test)
print("Training accuracy is {:.2f}".format(accuracy_train_01))
print("Testing accuracy is {:.2f}".format(accuracy_test_01))

predictions = svc.predict(x_test)
print(predictions)

# --------------------------Random Forest -----------------------------------
from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier(n_estimators=22)
rfc.fit(x_train, y_train)
accuracy_train_02=svc.score(x_train, y_train)
accuracy_test_02=svc.score(x_test, y_test)
print("Training accuracy is {:.2f}".format(accuracy_train_02))
print("Testing accuracy is {:.2f}".format(accuracy_test_02))

# -------------------------Neural Network Classifier---------------------------------
from sklearn.neural_network import MLPClassifier
nnc=MLPClassifier(hidden_layer_sizes=(7), activation="relu", max_iter=10000)

nnc.fit(x_train, y_train)

accuracy_train_03=svc.score(x_train, y_train)
accuracy_test_03=svc.score(x_test, y_test)
print("Training accuracy is {:.2f}".format(accuracy_train_03))
print("Testing accuracy is {:.2f}".format(accuracy_test_03))

predictions = nnc.predict(x_test)
print(predictions)

# ---------------------------Principle Component Analysis------------------------------
df.keys()

# ------------------ Scaling the values before PCA ------------------------
from sklearn.preprocessing import StandardScaler
scaler= StandardScaler()
xScaledTrain=scaler.fit_transform(x_train)

df.shape

from sklearn.decomposition import PCA 
pca = PCA(n_components=5)

principal_components= pca.fit_transform(xScaledTrain)
print(principal_components)

pca.explained_variance_ratio_

sum(pca.explained_variance_ratio_)

principal_df = pd.DataFrame(data=principal_components, columns=["principle_component_1", "principle_component_2","3","4","5"])
main_df=pd.concat([principal_df, df[['benign_malignant']]], axis=1)
principal_df.head()

main_df = main_df.dropna(how='any')

xPCA=main_df.drop('benign_malignant',axis=1)
yPCA=main_df[['benign_malignant']]
x_train, x_test, y_train, y_test = train_test_split(xPCA, yPCA,stratify=yPCA, train_size=.8,random_state=42)
main_df.head()

#-------------------------Support Vector Classifier after PCA -----------------------
from sklearn.svm import SVC
svc = SVC(kernel="linear")
svc.fit(x_train, y_train)
accuracy_train_04=svc.score(x_train, y_train)
accuracy_test_04=svc.score(x_test, y_test)
print("Training accuracy is {:.2f}".format(svc.score(x_train, y_train)))
print("Testing accuracy is {:.2f}".format(svc.score(x_test, y_test)))

#-----------------------------Random Forest after PCA-----------------------------------
from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier(n_estimators=50)
rfc.fit(x_train, y_train)
rfc.predict(x_test)
rfc.score(x_test,y_test)
accuracy_train_05=svc.score(x_train, y_train)
accyracy_test_05=svc.score(x_test, y_test)
print("Training accuracy is {:.2f}".format(svc.score(x_train, y_train)))
print("Testing accuracy is {:.2f}".format(svc.score(x_test, y_test)))

# ------------------Neural Network Classifier after PCA---------------------------
from sklearn.neural_network import MLPClassifier
nnc=MLPClassifier(hidden_layer_sizes=(7), activation="relu", max_iter=6000)
nnc.fit(x_train, y_train)
accuracy_train_06=svc.score(x_train, y_train)
accuracy_test_06=svc.score(x_test, y_test)
print("Training accuracy is {:.2f}".format(svc.score(x_train, y_train)))
print("Testing accuracy is {:.2f}".format(svc.score(x_test, y_test)))

#-------------------- Bar Chart Comparison -----------------
fig, ax= plt.subplots()
names= ['SVC', 'Random Forest', 'NNC']
accuracy=[accuracy_train_01,accuracy_train_02,accuracy_train_03]
accuracy_2=[accuracy_train_04,accuracy_train_05,accuracy_train_06]
position=[1,2,3]
position2=[1.3,2.3,3.3]
ax.bar(position,accuracy, width=0.3, color="red", bottom=None, align='center')
ax.bar(position2,accuracy_2, width=0.3, color="orange", bottom=None, align='center')
plt.xticks(position2,names)
ax.set_title('Comparison of accuracy between SVC, Random Forest and NNC before and after PCA\n Red = Before, Orange = After')
ax.set_xlabel('Classification')
ax.set_ylabel('Accuracy')